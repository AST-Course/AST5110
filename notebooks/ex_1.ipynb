{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Discretization and Finite Differences\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this exercise, you will:\n",
    "- Understand how to discretize a continuous function on a numerical grid\n",
    "- Implement forward difference approximations for derivatives\n",
    "- Analyze the order of convergence of numerical schemes\n",
    "- Investigate the effects of floating-point precision on numerical accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Discretization and Numerical Differentiation\n",
    "\n",
    "### The Test Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$h(x) = \\cos\\left[\\frac{\\pi (x-1)}{2}\\right] \\exp\\left[-\\left(\\frac{x-3}{2.5}\\right)^2\\right],\\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $x \\in (-4,10)$.\n",
    "\n",
    "### Setting Up the Grid\n",
    "\n",
    "Let us define a sampling of $h$ with 64 intervals (65 points), storing values into double precision arrays `xx` and `hh`. NumPy arrays use double precision (`float64`) by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example code to define `xx`:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "nump = 65\n",
    "x0 = -4.0 \n",
    "xf = 10.0\n",
    "xx = np.arange(nump) / (nump - 1.0) * (xf - x0) + x0\n",
    "```\n",
    "\n",
    "Use `matplotlib.pyplot` to visualize `hh` vs `xx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Forward Difference\n",
    "\n",
    "Define `nint` as the number of intervals (`nint = 64`) and `nump` as the number of points. In Python, array indices go from `0` to `nump-1`.\n",
    "\n",
    "Implement the forward difference formula from the [wiki](https://github.com/AST-Course/AST5110/wiki/Discretization) by completing the function `deriv_fwd` in [`nm_lib`](https://github.com/AST-Course/nm_lib/tree/master/nm_lib/nm_ex/nm_lib_ex_1.py).\n",
    "\n",
    "> **Note:** Depending on your implementation, you may have `nump` or `nump-1` elements. If the former, the last component (`nump-1`) is ill-calculated. The array `hp` contains a second-order approximation to the derivative at the intermediate points $x_{i+1/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "### Task 1.1: Visualize the Discretized Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot `hh` versus `xx` as a solid line with crosses at each grid point (to visualize the quality of the discretization), or use `plt.step` combined with `plt.plot`.\n",
    "\n",
    "> **Hint:** Make sure the axis pixels are properly located either at the center or half-grid shifted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Compare Numerical and Analytical Derivatives\n",
    "\n",
    "Plot the numerical derivative array `hp`. Calculate the analytical derivative of function (1) and plot it in the same figure to assess the quality of the approximation.\n",
    "\n",
    "> **Hint:** Make sure the axis pixels are properly located either at the center or half-grid shifted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Resolution Study\n",
    "\n",
    "Repeat the analysis with different resolutions:\n",
    "- **Coarser grids:** `nint = 32` and `nint = 16` — observe how the approximation deteriorates\n",
    "- **Finer grids:** `nint = 128` and `nint = 256` — observe how it improves\n",
    "\n",
    "> **Tip:** Consider using `plt.semilogy` to visualize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Testing the Order of Convergence\n",
    "\n",
    "### Task 2.1: Convergence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether the ratio $(h_{i+1}-h_i)/(x_{i+1}-x_i)$ approaches the analytical derivative value.\n",
    "\n",
    "**Steps:**\n",
    "1. Use samplings with 16, 32, 64, 128, 256, 512, 1024 intervals (successive powers of 2), plus $10^5$, $10^6$, and $2 \\times 10^6$\n",
    "2. Calculate the **maximum absolute error** between analytical and numerical derivatives at the same points\n",
    "3. Plot error vs. interval size using **logarithmic axes**\n",
    "4. Verify if the curve shows a **quadratic dependence**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Fit the Convergence Rate\n",
    "\n",
    "Fit a straight line to the logarithm of the error curve using `numpy.polyfit` and `numpy.poly1d`. From the slope, determine how accurately you obtain the expected quadratic dependence.\n",
    "\n",
    "> **Note:** Only fit in the region that is linear in the log-log plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Machine Precision Effects\n",
    "\n",
    "**Questions to explore:**\n",
    "1. What happens with $10^6$ grid points?\n",
    "2. What happens with different floating-point precisions?\n",
    "\n",
    "Test with:\n",
    "- `np.float32` (single precision)\n",
    "- `np.float64` (double precision — NumPy default)\n",
    "- `np.float128` (extended precision)\n",
    "\n",
    "> **Warning:** `np.float128` sometimes causes errors with `np.roll`!\n",
    "\n",
    "Explain the error slopes you observe. What is happening with `np.float128`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Analytical Proof of Convergence Order (Optional)\n",
    "\n",
    "### Taylor Expansion Analysis\n",
    "\n",
    "Consider uniform grid spacing $(\\Delta x)_i = \\Delta x$. Write Taylor expansions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_{i+1}) = f(x_{i+1/2}) + f'(x_{i+1/2})\\frac{\\Delta x}{2} + ...  \\tag{2}$$\n",
    "\n",
    "$$f(x_{i}) = f(x_{i+1/2}) - f'(x_{i+1/2})\\frac{\\Delta x}{2} + ...  \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include terms up to order $(\\Delta x)^3$. By combining these expressions and eliminating terms, prove that the finite-difference approximation to the derivative at midpoints $x_{i+1/2}$ is **2nd order accurate**.\n",
    "\n",
    "What is the accurate precission for the finite-difference approximation to the derivative at midpoints $x_{i}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Fourth-Order Formula (Optional)\n",
    "\n",
    "### Task 4.1: Implement Fourth-Order Scheme\n",
    "\n",
    "Implement the finite difference formula given by the first term on the right of Eq. (6) in the [wiki](https://github.com/AST-Course/AST5110/wiki/Discretization). Test if you obtain an approximation with error converging like $(\\Delta x)^4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Compare Convergence Rates\n",
    "\n",
    "Compare the 2nd-order and 4th-order error slopes for grids with 16, 32, 64, 128, 256, ... intervals.\n",
    "\n",
    "---\n",
    "\n",
    "## Hints\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 1: Calculating the analytical derivative</b></summary>\n",
    "\n",
    "For $h(x) = \\cos\\left[\\frac{\\pi (x-1)}{2}\\right] \\exp\\left[-\\left(\\frac{x-3}{2.5}\\right)^2\\right]$, use the product rule:\n",
    "$$h'(x) = \\frac{d}{dx}[\\cos(...)] \\cdot \\exp(...) + \\cos(...) \\cdot \\frac{d}{dx}[\\exp(...)]$$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 2: Log-log plot interpretation</b></summary>\n",
    "\n",
    "In a log-log plot, a power law $\\text{error} \\propto (\\Delta x)^m$ appears as a straight line with slope $m$.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>Hint 3: Machine precision floor</b></summary>\n",
    "\n",
    "At very fine resolutions, round-off errors dominate and the error stops decreasing. This happens around $\\epsilon_{\\text{machine}} / \\Delta x$.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
